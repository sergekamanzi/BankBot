{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergekamanzi/BankBot/blob/main/chatbot3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install rapidfuzz torch pandas numpy transformers scikit-learn nltk"
      ],
      "metadata": {
        "id": "4MQbbNbcJ_eb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Importing all necessary libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "import rapidfuzz\n",
        "import re\n",
        "import os\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set Seaborn style for better visuals\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Download NLTK data for BLEU score\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK resources: {e}\")\n",
        "    print(\"Please ensure NLTK resources 'punkt' and 'punkt_tab' are available.\")\n",
        "    exit(1)"
      ],
      "metadata": {
        "id": "EIx90fSnaOH_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "kR2AgH6PjUDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This cell loads the dataset, handles missing values, checks for duplicates and short texts,\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/banking.csv\")\n",
        "    df = df.dropna(subset=[\"question\", \"response\"])\n",
        "    # Check for duplicates and short texts\n",
        "    duplicate_questions = df[\"question\"].duplicated().sum()\n",
        "    short_responses = df[\"response\"].str.len().lt(10).sum()\n",
        "    if duplicate_questions > 0:\n",
        "        print(f\"Warning: Found {duplicate_questions} duplicate questions in banking.csv. Consider removing duplicates.\")\n",
        "    if short_responses > 0:\n",
        "        print(f\"Warning: Found {short_responses} responses shorter than 10 characters. Short texts may lower BLEU scores.\")\n",
        "    # Split dataset: 80% train, 20% test\n",
        "    train_size = int(0.8 * len(df))\n",
        "    train_df = df[:train_size]\n",
        "    test_df = df[train_size:]\n",
        "    train_questions = train_df[\"question\"].tolist()\n",
        "    train_responses = train_df[\"response\"].tolist()\n",
        "    test_questions = test_df[\"question\"].tolist()\n",
        "    test_responses = test_df[\"response\"].tolist()\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: banking.csv file not found. Please check the file path.\")\n",
        "    exit(1)\n",
        "except KeyError:\n",
        "    print(\"Error: 'question' or 'response' columns not found in banking.csv.\")\n",
        "    exit(1)\n",
        "\n",
        "# Check test set size\n",
        "if len(test_questions) < 5:\n",
        "    print(f\"Warning: Test set has only {len(test_questions)} samples. Metrics may be unreliable. Consider increasing dataset size or using a 90/10 split.\")"
      ],
      "metadata": {
        "id": "P-QzYSrvePjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Creation for Fine-Tuning"
      ],
      "metadata": {
        "id": "jgytGekVjbf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This cell defines a custom Dataset class for preparing question-response pairs for model fine-tuning\n",
        "class BankingDataset(Dataset):\n",
        "    def __init__(self, questions, responses, tokenizer, max_length=128):\n",
        "        self.questions = questions\n",
        "        self.responses = responses\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        response = self.responses[idx]\n",
        "        # Create a negative response by selecting a random different response\n",
        "        negative_response = random.choice([r for i, r in enumerate(self.responses) if i != idx])\n",
        "\n",
        "        # Tokenize question, positive response, and negative response\n",
        "        q_inputs = self.tokenizer(question, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "        p_inputs = self.tokenizer(response, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "        n_inputs = self.tokenizer(negative_response, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "\n",
        "        return {\n",
        "            'question': {k: v.squeeze(0) for k, v in q_inputs.items()},\n",
        "            'positive': {k: v.squeeze(0) for k, v in p_inputs.items()},\n",
        "            'negative': {k: v.squeeze(0) for k, v in n_inputs.items()}\n",
        "        }"
      ],
      "metadata": {
        "id": "rLwHkjaEjYin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoding (Normalization)"
      ],
      "metadata": {
        "id": "AqY6GV2gjid1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell encodes questions into embeddings for efficient similarity computation\n",
        "def encode_questions(questions, tokenizer, model, batch_size=16):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    for i in range(0, len(questions), batch_size):\n",
        "        batch = questions[i:i + batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu()\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "KvAce99ojjec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-Tuning the Model"
      ],
      "metadata": {
        "id": "Q3tGgzrkjuP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This cell fine-tunes the transformer model using contrastive loss on question-response pairs\n",
        "def fine_tune_model(model, tokenizer, questions, responses, epochs=3, batch_size=8, learning_rate=2e-5):\n",
        "    dataset = BankingDataset(questions, responses, tokenizer)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = torch.nn.CosineEmbeddingLoss(margin=0.5)\n",
        "\n",
        "    model.train()\n",
        "    print(\"Fine-tuning model...\")\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            # Move batch to device\n",
        "            q_inputs = {k: v.to(device) for k, v in batch['question'].items()}\n",
        "            p_inputs = {k: v.to(device) for k, v in batch['positive'].items()}\n",
        "            n_inputs = {k: v.to(device) for k, v in batch['negative'].items()}\n",
        "\n",
        "            # Forward pass\n",
        "            q_outputs = model(**q_inputs).last_hidden_state.mean(dim=1)\n",
        "            p_outputs = model(**p_inputs).last_hidden_state.mean(dim=1)\n",
        "            n_outputs = model(**n_inputs).last_hidden_state.mean(dim=1)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            positive_loss = loss_fn(q_outputs, p_outputs, torch.ones(q_outputs.size(0)).to(device))\n",
        "            negative_loss = loss_fn(q_outputs, n_outputs, torch.full((q_outputs.size(0),), -1).to(device))\n",
        "            loss = positive_loss + negative_loss\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    # Save fine-tuned model\n",
        "    save_path = \"./fine_tuned_model\"\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "    print(f\"Fine-tuned model saved to {save_path}\")"
      ],
      "metadata": {
        "id": "eNuIJ_fIjsGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Testing (Evaluation)"
      ],
      "metadata": {
        "id": "FYIfe6wzj4E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This cell evaluates the model on the test set, computing accuracy, F1, and BLEU scores\n",
        "def evaluate_model(model, tokenizer, questions, responses, test_questions, test_responses, batch_size=16, threshold=0.4):\n",
        "    model.eval()\n",
        "    embeddings = encode_questions(questions, tokenizer, model, batch_size)\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    bleu_scores = []\n",
        "    similarities = []\n",
        "    top_k_indices = []\n",
        "\n",
        "    print(\"Evaluating model...\")\n",
        "    for idx, test_q in enumerate(test_questions):\n",
        "        # Encode test question\n",
        "        inputs = tokenizer(test_q, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            user_emb = model(**inputs).last_hidden_state.mean(dim=1).cpu()\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        scores = [torch.nn.functional.cosine_similarity(user_emb, q_emb.unsqueeze(0)).item() for q_emb in embeddings]\n",
        "        best_idx = scores.index(max(scores))\n",
        "        best_score = max(scores)\n",
        "        similarities.append(best_score)\n",
        "\n",
        "        # Get top-5 indices for top-K accuracy\n",
        "        top_5_indices = np.argsort(scores)[-5:][::-1]\n",
        "        top_k_indices.append(top_5_indices)\n",
        "\n",
        "        # Map test question to closest train question\n",
        "        true_train_idx = min(range(len(questions)), key=lambda i: rapidfuzz.fuzz.ratio(test_q.lower(), questions[i].lower()))\n",
        "        y_true.append(true_train_idx)\n",
        "        y_pred.append(best_idx if best_score >= threshold else -1)\n",
        "\n",
        "        # BLEU score: compare predicted response to true response\n",
        "        true_response = test_responses[idx]\n",
        "        pred_response = responses[best_idx] if best_score >= threshold else \"\"\n",
        "        try:\n",
        "            true_tokens = nltk.word_tokenize(true_response.lower())\n",
        "            pred_tokens = nltk.word_tokenize(pred_response.lower()) if pred_response else []\n",
        "            if pred_tokens and true_tokens:\n",
        "                bleu = sentence_bleu([true_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method1)\n",
        "            else:\n",
        "                bleu = 0.0 if true_response != pred_response else 1.0  # Handle identical empty or same responses\n",
        "            bleu_scores.append(bleu)\n",
        "            print(f\"Test Question {idx}: '{test_q}'\")\n",
        "            print(f\"True Response: '{true_response}'\")\n",
        "            print(f\"Predicted Response: '{pred_response}'\")\n",
        "            print(f\"Similarity Score: {best_score:.4f}, BLEU: {bleu:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: BLEU calculation failed for index {idx}: {e}\")\n",
        "            bleu_scores.append(0.0)\n",
        "\n",
        "    # Print similarity distribution\n",
        "    print(f\"Similarity Scores: Min={min(similarities):.4f}, Max={max(similarities):.4f}, Mean={np.mean(similarities):.4f}\")\n",
        "    suggested_threshold = np.percentile(similarities, 25)\n",
        "    print(f\"Suggested Threshold: {suggested_threshold:.4f} (25th percentile of similarities)\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    binary_predictions = [1 if y_pred[i] != -1 and y_pred[i] == y_true[i] else 0 for i in range(len(y_true))]\n",
        "    binary_labels = [1 if y_pred[i] != -1 and y_pred[i] == y_true[i] else 0 for i in range(len(y_true))]\n",
        "\n",
        "    accuracy = accuracy_score(binary_predictions, binary_labels) if binary_predictions else 0.0\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(binary_predictions, binary_labels, average='weighted', zero_division=0)\n",
        "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
        "\n",
        "    return accuracy, f1, avg_bleu, similarities, bleu_scores, top_k_indices, y_true"
      ],
      "metadata": {
        "id": "QOOdFQNCj18t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization (Graphing)"
      ],
      "metadata": {
        "id": "T_Uhp4Baj-_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell generates three plots: similarity score distribution, BLEU scores, and top-K accuracy\n",
        "def plot_similarity_distribution(similarities, threshold):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(similarities, bins=20, kde=True, color='blue')\n",
        "    plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold ({threshold})')\n",
        "    plt.title('Distribution of Cosine Similarity Scores')\n",
        "    plt.xlabel('Cosine Similarity')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_bleu_scores(bleu_scores):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(range(len(bleu_scores)), bleu_scores, color='green')\n",
        "    plt.title('BLEU Scores per Test Question')\n",
        "    plt.xlabel('Test Question Index')\n",
        "    plt.ylabel('BLEU Score')\n",
        "    plt.xticks(range(len(bleu_scores)))\n",
        "    plt.show()\n",
        "\n",
        "def plot_top_k_accuracy(top_k_indices, y_true):\n",
        "    k_values = [1, 3, 5]\n",
        "    accuracies = []\n",
        "    for k in k_values:\n",
        "        correct = sum(1 for i, true_idx in enumerate(y_true) if true_idx in top_k_indices[i][:k])\n",
        "        accuracy = correct / len(y_true) if y_true else 0.0\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(x=k_values, y=accuracies, marker='o', color='purple')\n",
        "    plt.title('Top-K Retrieval Accuracy')\n",
        "    plt.xlabel('K (Top-K Matches)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(k_values)\n",
        "    plt.ylim(0, 1)\n",
        "    for i, acc in enumerate(accuracies):\n",
        "        plt.text(k_values[i], acc + 0.02, f'{acc:.4f}', ha='center')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "snfbjb8Bj6yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Script and Chatbot Logic"
      ],
      "metadata": {
        "id": "D27xToT5kHUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell initializes the model, runs the fine-tuning and evaluation, and implements the chatbot logic\n",
        "if __name__ == \"__main__\":\n",
        "    # Load model and tokenizer\n",
        "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Fine-tune the model\n",
        "    fine_tune_model(model, tokenizer, train_questions, train_responses)\n",
        "\n",
        "    # Load fine-tuned model\n",
        "    model = AutoModel.from_pretrained(\"./fine_tuned_model\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Generate embeddings with fine-tuned model\n",
        "    question_embeddings = encode_questions(train_questions, tokenizer, model)\n",
        "\n",
        "    # Evaluate model\n",
        "    accuracy, f1, avg_bleu, similarities, bleu_scores, top_k_indices, y_true = evaluate_model(\n",
        "        model, tokenizer, train_questions, train_responses, test_questions, test_responses\n",
        "    )\n",
        "\n",
        "    # Generate graphs\n",
        "    plot_similarity_distribution(similarities, threshold=0.4)\n",
        "    plot_bleu_scores(bleu_scores)\n",
        "    plot_top_k_accuracy(top_k_indices, y_true)\n",
        "\n",
        "    # Memory & context\n",
        "    user_memory = {\"name\": None, \"location\": None}\n",
        "    last_topic = None\n",
        "\n",
        "    # Variations\n",
        "    greetings = [\"hi\", \"hello\", \"hey\", \"good morning\", \"good afternoon\", \"howdy\", \"greetings\", \"what's up\", \"yo\", \"hiya\"]\n",
        "    thanks = [\"thank you\", \"thanks\", \"thx\", \"appreciate\", \"much obliged\", \"cheers\", \"thankful\", \"ty\", \"gracias\", \"merci\"]\n",
        "    affirmatives = [\"yes\", \"yeah\", \"sure\", \"ok\", \"alright\", \"definitely\", \"of course\", \"yea\", \"yup\", \"certainly\"]\n",
        "    off_topic_keywords = [\"food\", \"love\", \"music\", \"football\", \"weather\", \"play\", \"movie\", \"hiking\", \"robot\", \"interact\"]\n",
        "\n",
        "    # Domain-specific keywords and responses\n",
        "    domain_keywords = {\n",
        "        \"profit\": \"Profit and loss topics are best discussed with a financial advisor. I'm happy to assist with banking needs like savings or cards.\"\n",
        "    }\n",
        "\n",
        "    # Dynamic responses\n",
        "    greet_replies = [\n",
        "        \"Hey {name}, how can I help you today?\", \"Nice to see you, {name}. What can I do for you?\",\n",
        "        \"Hi {name}! Ready to explore some banking options?\", \"Hello {name}, let's take care of your banking needs.\",\n",
        "        \"Welcome back, {name}! How can I assist?\", \"Hello there, {name}. What would you like help with today?\",\n",
        "        \"Hi {name}, what banking task are we tackling today?\", \"Hey {name}, how can I support you?\",\n",
        "        \"Hey {name}, let's sort your banking needs.\", \"Greetings {name}, happy to help!\"\n",
        "    ]\n",
        "\n",
        "    thank_replies = [\n",
        "        \"You're welcome!\", \"Happy to help!\", \"Anytime!\", \"No problem, {name}!\",\n",
        "        \"Glad to assist!\", \"Always here to help!\", \"You're most welcome!\", \"With pleasure!\",\n",
        "        \"No worries!\", \"Sure thing, {name}!\"\n",
        "    ]\n",
        "\n",
        "    off_topic_replies = [\n",
        "        \"Haha, that’s fun! I focus on banking. Want to check your balance or report a card issue?\",\n",
        "        \"Interesting! I specialize in banking. Would you like help with loans or ATM info?\",\n",
        "        \"Cool! I'm trained for banking help. Try asking about your account or a transaction.\",\n",
        "        \"That's a fun topic! I'm focused on banking though. Need help with cards or savings?\",\n",
        "        \"I love that! Let’s talk banking—need help with something like fraud or PINs?\",\n",
        "        \"Nice one! My zone is banking. Want help opening an account?\",\n",
        "        \"Haha, I feel you! I handle account inquiries and loans best.\",\n",
        "        \"That’s outside my expertise. Want to know your balance instead?\",\n",
        "        \"Sounds exciting! But I’m more into balances and transfers!\",\n",
        "        \"I wish I could help with that! But I’m your banking assistant\"\n",
        "    ]\n",
        "\n",
        "    fallback_replies = [\n",
        "        \"I'm here for your banking needs—like loans, fraud, or PIN help. What do you need?\",\n",
        "        \"Sorry, I didn’t quite get that. Would you like help with opening an account?\",\n",
        "        \"Interesting! I focus on banking. Try asking about ATM, transfers, or cards.\",\n",
        "        \"Hmm, I’m not sure. But I’m great at deposits, balances, and reports!\",\n",
        "        \"I’m better at banking questions. Want to check your balance or card status?\",\n",
        "        \"Could you rephrase that? I can help with things like savings or fraud issues.\",\n",
        "        \"Let’s get back to banking. Do you want to know about transfers or cards?\",\n",
        "        \"Sorry, didn’t follow. But I can help with loans, accounts, or deposits.\",\n",
        "        \"That’s a bit unclear. Do you need help with your account or a transaction?\",\n",
        "        \"Let’s stay on topic—banking help like ATM, PIN, or transfers coming up?\"\n",
        "    ]\n",
        "\n",
        "    # Helper functions\n",
        "    def correct_spelling(text):\n",
        "        return str(TextBlob(text).correct())\n",
        "\n",
        "    def update_memory(text):\n",
        "        global user_memory\n",
        "        name_match = re.search(r\"(my name is|i am|i'm|this is|call me|you can call me|it's|they call me|name's)\\s+(\\w+)\", text.lower())\n",
        "        name_correction = re.search(r\"not (\\w+)\", text.lower())\n",
        "        loc_match = re.search(r\"(i live in|i'm from|i stay in)\\s+([a-zA-Z\\s]+)\", text.lower())\n",
        "\n",
        "        response = \"\"\n",
        "        if name_match:\n",
        "            user_memory[\"name\"] = name_match.group(2).capitalize()\n",
        "            response += f\"Nice to meet you, {user_memory['name']}! \"\n",
        "        elif name_correction and user_memory[\"name\"]:\n",
        "            corrected_name = name_correction.group(1).capitalize()\n",
        "            user_memory[\"name\"] = corrected_name\n",
        "            response += f\"Got it! I’ve updated your name to {corrected_name}. \"\n",
        "\n",
        "        if loc_match:\n",
        "            user_memory[\"location\"] = loc_match.group(2).strip().capitalize()\n",
        "            response += f\"{user_memory['location']} sounds like a great place. \"\n",
        "\n",
        "        if response:\n",
        "            response += \"How can I assist you with banking today?\"\n",
        "            return response\n",
        "        return None\n",
        "\n",
        "    def personalize(text):\n",
        "        return text.replace(\"{name}\", user_memory[\"name\"] if user_memory[\"name\"] else \"there\")\n",
        "\n",
        "    def detect_intent(text):\n",
        "        text = text.lower()\n",
        "        for g in greetings:\n",
        "            if rapidfuzz.fuzz.partial_ratio(g, text) > 90:\n",
        "                return \"greet\"\n",
        "        for t in thanks:\n",
        "            if rapidfuzz.fuzz.partial_ratio(t, text) > 90:\n",
        "                return \"thanks\"\n",
        "        for w in off_topic_keywords:\n",
        "            if w in text:\n",
        "                return \"off_topic\"\n",
        "        if \"remember\" in text and \"name\" in text:\n",
        "            return \"check_name\"\n",
        "        if any(word in text for word in affirmatives):\n",
        "            return \"confirm\"\n",
        "        if \"talk like a human\" in text or \"interact like a human\" in text:\n",
        "            return \"human_mode\"\n",
        "        if \"more about\" in text or \"tell me more\" in text:\n",
        "            return \"clarify\"\n",
        "        return \"question\"\n",
        "\n",
        "    # Main chatbot loop\n",
        "    def chatbot():\n",
        "        global last_topic\n",
        "        print(\"🤖 Human-Like Banking Chatbot is ready! Type 'exit' to quit.\\n\")\n",
        "        while True:\n",
        "            user_input = input(\"You: \").strip()\n",
        "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "                print(\"Chatbot: Goodbye! It was a pleasure assisting you.\")\n",
        "                break\n",
        "            if not user_input:\n",
        "                print(\"Chatbot: Please type something to continue!\")\n",
        "                continue\n",
        "\n",
        "            corrected = correct_spelling(user_input)\n",
        "            memory_reply = update_memory(corrected)\n",
        "            if memory_reply:\n",
        "                print(\"Chatbot:\", memory_reply)\n",
        "                continue\n",
        "\n",
        "            for keyword in domain_keywords:\n",
        "                if keyword in corrected.lower():\n",
        "                    print(\"Chatbot:\", domain_keywords[keyword])\n",
        "                    continue\n",
        "\n",
        "            intent = detect_intent(corrected)\n",
        "\n",
        "            if intent == \"greet\":\n",
        "                print(\"Chatbot:\", personalize(random.choice(greet_replies)))\n",
        "                continue\n",
        "            if intent == \"thanks\":\n",
        "                print(\"Chatbot:\", personalize(random.choice(thank_replies)))\n",
        "                continue\n",
        "            if intent == \"off_topic\":\n",
        "                print(\"Chatbot:\", random.choice(off_topic_replies))\n",
        "                continue\n",
        "            if intent == \"check_name\":\n",
        "                if user_memory[\"name\"]:\n",
        "                    print(f\"Chatbot: Yes! You told me your name is {user_memory['name']}.\")\n",
        "                else:\n",
        "                    print(\"Chatbot: I don’t think you’ve told me your name yet. Try saying 'My name is ...'.\")\n",
        "                continue\n",
        "            if intent == \"human_mode\":\n",
        "                print(\"Chatbot: Absolutely! I’m here to chat in a friendly way and support your banking needs 😊\")\n",
        "                continue\n",
        "            if intent == \"confirm\":\n",
        "                if last_topic:\n",
        "                    print(f\"Chatbot: Continuing from our last topic: **{last_topic}**. What exactly would you like to know?\")\n",
        "                else:\n",
        "                    print(\"Chatbot: Sure! Can you share more about what you'd like help with?\")\n",
        "                continue\n",
        "            if intent == \"clarify\":\n",
        "                if last_topic:\n",
        "                    print(f\"Chatbot: Here's more on your last topic: **{last_topic}**. Do you want to open an account or understand account types?\")\n",
        "                else:\n",
        "                    print(\"Chatbot: Can you clarify which topic you'd like to dive into more?\")\n",
        "                continue\n",
        "\n",
        "            # Semantic similarity\n",
        "            inputs = tokenizer(corrected, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "            with torch.no_grad():\n",
        "                user_emb = model(**inputs).last_hidden_state.mean(dim=1).cpu()\n",
        "            scores = [torch.nn.functional.cosine_similarity(user_emb, q_emb.unsqueeze(0)).item() for q_emb in question_embeddings]\n",
        "            best_idx = scores.index(max(scores))\n",
        "            best_score = max(scores)\n",
        "\n",
        "            if best_score > 0.4:\n",
        "                last_topic = train_questions[best_idx]\n",
        "                print(\"Chatbot:\", personalize(train_responses[best_idx]))\n",
        "            else:\n",
        "                print(\"Chatbot:\", random.choice(fallback_replies))\n",
        "\n",
        "    # Run the chatbot\n",
        "    chatbot()"
      ],
      "metadata": {
        "id": "MQ1pk_YTkERj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}