{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergekamanzi/Chat-Bot-/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEqKqeym6h6i",
        "outputId": "8f7df5ac-eb87-45e6-c946-ccd44d4305de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Using cached gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.11 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the question-answer data\n",
        "data = {\n",
        "    \"What was the significance of 1957 in Rwanda?\": \"In 1957, the Hutu Manifesto was published by Hutu intellectuals demanding political reform and majority rule, marking a turning point in ethnic tensions.\",\n",
        "    \"What happened during the Rwandan Revolution of 1959?\": \"The Rwandan Revolution in 1959 led to the end of the Tutsi monarchy and the rise of a Hutu-led republic. It triggered ethnic violence and the exile of many Tutsis.\",\n",
        "    \"When did Rwanda gain independence?\": \"Rwanda gained independence from Belgium on July 1, 1962, with Grégoire Kayibanda becoming the first president.\",\n",
        "    \"Who was Grégoire Kayibanda?\": \"Grégoire Kayibanda was Rwanda's first president from 1962 to 1973. He led the country after independence and promoted Hutu nationalism.\",\n",
        "    \"What happened in 1973 in Rwanda?\": \"In 1973, Major General Juvénal Habyarimana overthrew Grégoire Kayibanda in a military coup and established a one-party state under the MRND party.\",\n",
        "    \"Who was Juvénal Habyarimana?\": \"Juvénal Habyarimana was Rwanda's second president, ruling from 1973 until his assassination in 1994. His regime was marked by ethnic favoritism and authoritarianism.\",\n",
        "    \"What caused the 1994 Genocide?\": \"The 1994 Genocide against the Tutsi was caused by long-standing ethnic tensions, political instability, and extremist propaganda. The assassination of President Habyarimana triggered the mass killings.\",\n",
        "    \"What happened during the 1994 Genocide in Rwanda?\": \"From April to July 1994, approximately 800,000 Tutsi and moderate Hutu were killed in a genocide perpetrated by Hutu extremists.\",\n",
        "    \"Who ended the 1994 Genocide in Rwanda?\": \"The Rwandan Patriotic Front (RPF), led by Paul Kagame, ended the genocide in July 1994 after capturing Kigali and taking control of the country.\",\n",
        "    \"What is the Rwandan Patriotic Front (RPF)?\": \"The RPF is a political and military organization that ended the 1994 Genocide and has led the Rwandan government since then. It was founded by Tutsi exiles in Uganda.\",\n",
        "    \"Who is Paul Kagame?\": \"Paul Kagame is Rwanda's current president and former military leader of the RPF. He played a key role in ending the 1994 Genocide and has been president since 2000.\",\n",
        "    \"What happened in Rwanda after the Genocide?\": \"After the Genocide, Rwanda focused on national unity, justice through Gacaca courts, reconstruction, and economic development under RPF leadership.\",\n",
        "    \"What are Gacaca courts in Rwanda?\": \"Gacaca courts were community-based courts used to try suspects of the 1994 Genocide, aimed at truth-telling, justice, and reconciliation.\",\n",
        "    \"When did Paul Kagame become president?\": \"Paul Kagame officially became president of Rwanda in 2000 after Pasteur Bizimungu resigned.\",\n",
        "    \"What has Rwanda achieved since 2000?\": \"Since 2000, Rwanda has made progress in healthcare, education, digital transformation, gender equality, and economic development.\",\n",
        "    \"What is Vision 2020 in Rwanda?\": \"Vision 2020 was a national development plan launched by the Rwandan government to transform the country into a middle-income economy by the year 2020.\",\n",
        "    \"What replaced Vision 2020 in Rwanda?\": \"Vision 2050 replaced Vision 2020 and aims to make Rwanda an upper-middle-income country by 2035 and a high-income country by 2050.\",\n",
        "    \"What is Umuganda in Rwanda?\": \"Umuganda is a monthly national community service day in Rwanda where citizens clean streets, plant trees, and build public infrastructure.\",\n",
        "    \"What is Rwanda known for today?\": \"Rwanda is known for its clean cities, rapid development, eco-tourism, commitment to reconciliation, and being one of the safest countries in Africa.\",\n",
        "    \"What is Kwibuka in Rwanda?\": \"Kwibuka is an annual commemoration of the 1994 Genocide against the Tutsi. It means 'to remember' in Kinyarwanda and begins every April 7.\"\n",
        "}\n",
        "\n",
        "# Load the sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extract questions and answers\n",
        "questions = list(data.keys())\n",
        "answers = list(data.values())\n",
        "\n",
        "# Encode the questions\n",
        "question_embeddings = model.encode(questions, convert_to_tensor=False)\n",
        "question_embeddings = np.array(question_embeddings).astype('float32')\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = question_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(question_embeddings)\n",
        "\n",
        "# Lists for detecting greetings, gratitude, farewells, emotions, and follow-ups\n",
        "GREETINGS = [\"hello\", \"hi\", \"hey\", \"greetings\", \"good morning\", \"good afternoon\", \"good evening\"]\n",
        "GRATITUDE = [\"thank you\", \"thanks\", \"appreciate it\", \"thank you so much\", \"thanks a lot\"]\n",
        "FAREWELLS = [\"bye\", \"goodbye\", \"see you\", \"farewell\", \"take care\"]\n",
        "FOLLOW_UPS = [\"tell me more\", \"more\", \"go on\", \"continue\"]\n",
        "\n",
        "# Emotion keyword lists\n",
        "HAPPINESS = [\"happy\", \"glad\", \"excited\", \"joyful\", \"pleased\"]\n",
        "SADNESS = [\"sad\", \"heartbreaking\", \"tragic\", \"depressing\", \"devastating\"]\n",
        "ANGER = [\"angry\", \"frustrated\", \"outraged\", \"mad\", \"furious\"]\n",
        "FEAR = [\"scared\", \"terrified\", \"afraid\", \"horrified\", \"fearful\"]\n",
        "SURPRISE = [\"shocked\", \"amazed\", \"surprised\", \"astonished\", \"stunned\"]\n",
        "\n",
        "def detect_emotion(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    if any(word in query_lower for word in HAPPINESS):\n",
        "        return \"happiness\", \"I’m glad to hear that brings you happiness.\"\n",
        "    elif any(word in query_lower for word in SADNESS):\n",
        "        return \"sadness\", \"I understand, that can be truly heartbreaking.\"\n",
        "    elif any(word in query_lower for word in ANGER):\n",
        "        return \"anger\", \"I can see why that might make you feel angry.\"\n",
        "    elif any(word in query_lower for word in FEAR):\n",
        "        return \"fear\", \"I can understand why that might feel terrifying.\"\n",
        "    elif any(word in query_lower for word in SURPRISE):\n",
        "        return \"surprise\", \"That’s quite surprising, isn’t it?\"\n",
        "    return None, \"\"\n",
        "\n",
        "def is_greeting(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(greeting in query_lower for greeting in GREETINGS)\n",
        "\n",
        "def is_gratitude(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(gratitude in query_lower for gratitude in GRATITUDE)\n",
        "\n",
        "def is_farewell(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(farewell in query_lower for farewell in FAREWELLS)\n",
        "\n",
        "def is_follow_up(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(follow_up in query_lower for follow_up in FOLLOW_UPS)\n",
        "\n",
        "# Prepare data for LSTM training\n",
        "all_text = questions + answers + [f\"User: {q} Bot: {a}\" for q, a in zip(questions, answers)]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_text)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create sequences for training (simplified example)\n",
        "sequences = tokenizer.texts_to_sequences(all_text)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "# Pad sequences for X\n",
        "X = pad_sequences(sequences[:-1], maxlen=max_length, padding='post')\n",
        "\n",
        "# Prepare y by taking the next word index for each sequence in X\n",
        "# This involves taking the token *after* the sequence in X.\n",
        "# We need to make sure the target sequences are not empty after slicing.\n",
        "y_sequences = [seq[max_length] if len(seq) > max_length else 0 for seq in sequences[1:]] # Get the next word index, handle shorter sequences\n",
        "\n",
        "# Use to_categorical on the next word indices\n",
        "y = tf.keras.utils.to_categorical(y_sequences, num_classes=vocab_size)\n",
        "\n",
        "\n",
        "# Build and train LSTM model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(vocab_size, 64, input_length=max_length),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# Ensure the shape of X matches the input_length\n",
        "# We need X to have shape (num_samples, max_length) and y to have shape (num_samples, vocab_size)\n",
        "# Check shapes before training\n",
        "# print(f\"Shape of X: {X.shape}\")\n",
        "# print(f\"Shape of y: {y.shape}\")\n",
        "lstm_model.fit(X, y, epochs=50, batch_size=1, verbose=0)  # Reduced verbosity for Colab\n",
        "\n",
        "def generate_lstm_response(history_text):\n",
        "    # Tokenize the history text\n",
        "    sequence = tokenizer.texts_to_sequences([history_text])\n",
        "    # Check if the sequence is empty or has length 0\n",
        "    if not sequence or not sequence[0]:\n",
        "        return \"I can provide more details. What topic would you like to explore?\"\n",
        "\n",
        "    # Pad the sequence\n",
        "    padded = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "    # Predict the next token\n",
        "    prediction = lstm_model.predict(padded, verbose=0)\n",
        "    predicted_index = np.argmax(prediction)\n",
        "    # Convert index back to word\n",
        "    word = tokenizer.index_word.get(predicted_index, \"details\") # Use .get with a default value\n",
        "    return word\n",
        "\n",
        "\n",
        "def chatbot(query, history):\n",
        "    # Initialize history if None\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    # Detect emotion\n",
        "    emotion, emotion_response = detect_emotion(query)\n",
        "\n",
        "    # Check for greetings\n",
        "    if is_greeting(query):\n",
        "        response = \"Hello! I'm here to assist you with questions about Rwandan history. What would you like to learn about today?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Check for gratitude\n",
        "    if is_gratitude(query):\n",
        "        response = \"You're welcome! I'm glad I could help. Would you like to know more about Rwandan history?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Check for farewells\n",
        "    if is_farewell(query):\n",
        "        response = \"Goodbye! Feel free to come back if you have more questions about Rwandan history.\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Check for follow-up questions\n",
        "    if is_follow_up(query) and history:\n",
        "        # Construct history text from the last turn\n",
        "        # This might be too simple for complex follow-ups, could consider last N turns\n",
        "        last_turn = history[-1]\n",
        "        history_text = last_turn[0] + \" \" + last_turn[1] # Combine user query and bot response from the last turn\n",
        "        lstm_response_word = generate_lstm_response(history_text)\n",
        "        # Generate a more coherent response using the predicted word\n",
        "        response = f\"Based on our conversation, perhaps you're interested in '{lstm_response_word}'. Would you like to know more about Rwandan history?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Encode the input query\n",
        "    query_embedding = model.encode([query], convert_to_tensor=False).astype('float32')\n",
        "\n",
        "    # Search for the closest question\n",
        "    k = 1  # Number of nearest neighbors\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Get the distance to the closest match\n",
        "    distance = distances[0][0]\n",
        "\n",
        "    # Set a threshold for out-of-domain detection\n",
        "    DISTANCE_THRESHOLD = 1.5 # This threshold might need tuning\n",
        "\n",
        "    if distance > DISTANCE_THRESHOLD:\n",
        "        response = \"I'm sorry, I don't have information on that topic. I can help with questions about Rwandan history. What would you like to know?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Get the most similar question and its answer\n",
        "    matched_question = questions[indices[0][0]]\n",
        "    matched_answer = answers[indices[0][0]]\n",
        "\n",
        "    # Add empathetic response if emotion is detected\n",
        "    if emotion:\n",
        "        response = f\"{emotion_response} Here’s what I found: {matched_answer} Would you like to know more about Rwandan history?\"\n",
        "    else:\n",
        "        response = f\"Here’s what I found: {matched_answer} Would you like to know more about Rwandan history?\"\n",
        "\n",
        "    history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "    return response, history\n",
        "\n",
        "# Create a Gradio interface with state to maintain history\n",
        "inputs = [\n",
        "    gr.Textbox(lines=2, placeholder=\"Ask a question about Rwandan history...\"),\n",
        "    gr.State(value=[])\n",
        "]\n",
        "outputs = [\n",
        "    gr.Textbox(label=\"Response\"),\n",
        "    gr.State()\n",
        "]\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    title=\"Rwandan History Chatbot\",\n",
        "    description=\"Ask questions about Rwandan history, and the chatbot will respond based on the provided knowledge base.\"\n",
        ")\n",
        "\n",
        "# Test the chatbot with sample queries\n",
        "test_queries = [\n",
        "    \"Hello\",  # Greeting\n",
        "    \"The 1994 genocide is so heartbreaking\",  # Sadness\n",
        "    \"Tell me more\",  # Follow-up\n",
        "    \"I’m so happy to learn about Rwanda’s progress\",  # Happiness\n",
        "    \"What happened in 1973 in Rwanda?\",  # In-domain\n",
        "    \"That’s shocking!\",  # Surprise\n",
        "    \"Thanks a lot\",  # Gratitude\n",
        "    \"Goodbye\",  # Farewell\n",
        "]\n",
        "\n",
        "print(\"Testing the chatbot with sample queries:\")\n",
        "history = []\n",
        "for query in test_queries:\n",
        "    response, history = chatbot(query, history)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {response}\\n\")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "0iUfT8zXvjYr",
        "outputId": "1f83d798-23f7-4c6a-e85a-04cac7f8aa25"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the chatbot with sample queries:\n",
            "Query: Hello\n",
            "Response: Hello! I'm here to assist you with questions about Rwandan history. What would you like to learn about today?\n",
            "\n",
            "Query: The 1994 genocide is so heartbreaking\n",
            "Response: I understand, that can be truly heartbreaking. Here’s what I found: The 1994 Genocide against the Tutsi was caused by long-standing ethnic tensions, political instability, and extremist propaganda. The assassination of President Habyarimana triggered the mass killings. Would you like to know more about Rwandan history?\n",
            "\n",
            "Query: Tell me more\n",
            "Response: Based on our conversation, perhaps you're interested in 'details'. Would you like to know more about Rwandan history?\n",
            "\n",
            "Query: I’m so happy to learn about Rwanda’s progress\n",
            "Response: I’m glad to hear that brings you happiness. Here’s what I found: Since 2000, Rwanda has made progress in healthcare, education, digital transformation, gender equality, and economic development. Would you like to know more about Rwandan history?\n",
            "\n",
            "Query: What happened in 1973 in Rwanda?\n",
            "Response: Here’s what I found: In 1973, Major General Juvénal Habyarimana overthrew Grégoire Kayibanda in a military coup and established a one-party state under the MRND party. Would you like to know more about Rwandan history?\n",
            "\n",
            "Query: That’s shocking!\n",
            "Response: I'm sorry, I don't have information on that topic. I can help with questions about Rwandan history. What would you like to know?\n",
            "\n",
            "Query: Thanks a lot\n",
            "Response: You're welcome! I'm glad I could help. Would you like to know more about Rwandan history?\n",
            "\n",
            "Query: Goodbye\n",
            "Response: Goodbye! Feel free to come back if you have more questions about Rwandan history.\n",
            "\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d1f53aed03a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;31m# Launch the Gradio interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m \u001b[0miface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[0m\n\u001b[1;32m   2848\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2849\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2850\u001b[0;31m                     share_url = networking.setup_tunnel(\n\u001b[0m\u001b[1;32m   2851\u001b[0m                         \u001b[0mlocal_host\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2852\u001b[0m                         \u001b[0mlocal_port\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_port\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/networking.py\u001b[0m in \u001b[0;36msetup_tunnel\u001b[0;34m(local_host, local_port, share_token, share_server_address, share_server_tls_certificate)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mshare_server_tls_certificate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     )\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0maddress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtunnel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_tunnel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/tunneling.py\u001b[0m in \u001b[0;36mstart_tunnel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_tunnel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_tunnel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBINARY_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/tunneling.py\u001b[0m in \u001b[0;36m_start_tunnel\u001b[0;34m(self, binary)\u001b[0m\n\u001b[1;32m    152\u001b[0m         )\n\u001b[1;32m    153\u001b[0m         \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_url_from_tunnel_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_url_from_tunnel_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/tunneling.py\u001b[0m in \u001b[0;36m_read_url_from_tunnel_stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCkJ1PJAvjV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g93uUn6yvjEJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}