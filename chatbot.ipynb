{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergekamanzi/Chat-Bot-/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install faiss-cpu pyspellchecker\n",
        "!pip install huggingface_hub[hf_xet]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEqKqeym6h6i",
        "outputId": "a77b5b2e-9603-472a-b689-c2cc9546d38d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.11)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.3\n",
            "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.11/dist-packages (0.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the question-answer data\n",
        "data = pd.read_csv(\"/content/rwanda_history.csv\")\n",
        "\n",
        "# Load the sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extract questions and answers\n",
        "questions = list(data.keys())\n",
        "answers = list(data.values())\n",
        "\n",
        "# Encode the questions\n",
        "question_embeddings = model.encode(questions, convert_to_tensor=False)\n",
        "question_embeddings = np.array(question_embeddings).astype('float32')\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = question_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(question_embeddings)\n",
        "\n",
        "# Lists for detecting greetings, gratitude, farewells, emotions, and follow-ups\n",
        "GREETINGS = [\"hello\", \"hi\", \"hey\", \"greetings\", \"good morning\", \"good afternoon\", \"good evening\"]\n",
        "GRATITUDE = [\"thank you\", \"thanks\", \"appreciate it\", \"thank you so much\", \"thanks a lot\"]\n",
        "FAREWELLS = [\"bye\", \"goodbye\", \"see you\", \"farewell\", \"take care\"]\n",
        "FOLLOW_UPS = [\"tell me more\", \"more\", \"go on\", \"continue\"]\n",
        "\n",
        "# Emotion keyword lists\n",
        "HAPPINESS = [\"happy\", \"glad\", \"excited\", \"joyful\", \"pleased\"]\n",
        "SADNESS = [\"sad\", \"heartbreaking\", \"tragic\", \"depressing\", \"devastating\"]\n",
        "ANGER = [\"angry\", \"frustrated\", \"outraged\", \"mad\", \"furious\"]\n",
        "FEAR = [\"scared\", \"terrified\", \"afraid\", \"horrified\", \"fearful\"]\n",
        "SURPRISE = [\"shocked\", \"amazed\", \"surprised\", \"astonished\", \"stunned\"]\n",
        "\n",
        "def detect_emotion(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    if any(word in query_lower for word in HAPPINESS):\n",
        "        return \"happiness\", \"I’m glad to hear that brings you happiness.\"\n",
        "    elif any(word in query_lower for word in SADNESS):\n",
        "        return \"sadness\", \"I understand, that can be truly heartbreaking.\"\n",
        "    elif any(word in query_lower for word in ANGER):\n",
        "        return \"anger\", \"I can see why that might make you feel angry.\"\n",
        "    elif any(word in query_lower for word in FEAR):\n",
        "        return \"fear\", \"I can understand why that might feel terrifying.\"\n",
        "    elif any(word in query_lower for word in SURPRISE):\n",
        "        return \"surprise\", \"That’s quite surprising, isn’t it?\"\n",
        "    return None, \"\"\n",
        "\n",
        "def is_greeting(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(greeting in query_lower for greeting in GREETINGS)\n",
        "\n",
        "def is_gratitude(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(gratitude in query_lower for gratitude in GRATITUDE)\n",
        "\n",
        "def is_farewell(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(farewell in query_lower for farewell in FAREWELLS)\n",
        "\n",
        "def is_follow_up(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(follow_up in query_lower for follow_up in FOLLOW_UPS)\n",
        "\n",
        "# Prepare data for LSTM training\n",
        "all_text = questions + answers + [f\"User: {q} Bot: {a}\" for q, a in zip(questions, answers)]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_text)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create sequences for training (simplified example)\n",
        "sequences = tokenizer.texts_to_sequences(all_text)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "# Pad sequences for X\n",
        "X = pad_sequences(sequences[:-1], maxlen=max_length, padding='post')\n",
        "\n",
        "# Prepare y by taking the next word index for each sequence in X\n",
        "# This involves taking the token *after* the sequence in X.\n",
        "# We need to make sure the target sequences are not empty after slicing.\n",
        "y_sequences = [seq[max_length] if len(seq) > max_length else 0 for seq in sequences[1:]] # Get the next word index, handle shorter sequences\n",
        "\n",
        "# Use to_categorical on the next word indices\n",
        "y = tf.keras.utils.to_categorical(y_sequences, num_classes=vocab_size)\n",
        "\n",
        "\n",
        "# Build and train LSTM model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(vocab_size, 64, input_length=max_length),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# Ensure the shape of X matches the input_length\n",
        "# We need X to have shape (num_samples, max_length) and y to have shape (num_samples, vocab_size)\n",
        "# Check shapes before training\n",
        "# print(f\"Shape of X: {X.shape}\")\n",
        "# print(f\"Shape of y: {y.shape}\")\n",
        "lstm_model.fit(X, y, epochs=50, batch_size=1, verbose=0)  # Reduced verbosity for Colab\n",
        "\n",
        "def generate_lstm_response(history_text):\n",
        "    # Tokenize the history text\n",
        "    sequence = tokenizer.texts_to_sequences([history_text])\n",
        "    # Check if the sequence is empty or has length 0\n",
        "    if not sequence or not sequence[0]:\n",
        "        return \"I can provide more details. What topic would you like to explore?\"\n",
        "\n",
        "    # Pad the sequence\n",
        "    padded = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "    # Predict the next token\n",
        "    prediction = lstm_model.predict(padded, verbose=0)\n",
        "    predicted_index = np.argmax(prediction)\n",
        "    # Convert index back to word\n",
        "    word = tokenizer.index_word.get(predicted_index, \"details\") # Use .get with a default value\n",
        "    return word\n",
        "\n",
        "\n",
        "def chatbot(query, history):\n",
        "    # Initialize history if None\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    # Detect emotion\n",
        "    emotion, emotion_response = detect_emotion(query)\n",
        "\n",
        "    # Check for greetings\n",
        "    if is_greeting(query):\n",
        "        response = \"Hello! I'm here to assist you with questions about Rwandan history. What would you like to learn about today?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Check for gratitude\n",
        "    if is_gratitude(query):\n",
        "        response = \"You're welcome! I'm glad I could help. Would you like to know more about Rwandan history?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Check for farewells\n",
        "    if is_farewell(query):\n",
        "        response = \"Goodbye! Feel free to come back if you have more questions about Rwandan history.\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Check for follow-up questions\n",
        "    if is_follow_up(query) and history:\n",
        "        # Construct history text from the last turn\n",
        "        # This might be too simple for complex follow-ups, could consider last N turns\n",
        "        last_turn = history[-1]\n",
        "        history_text = last_turn[0] + \" \" + last_turn[1] # Combine user query and bot response from the last turn\n",
        "        lstm_response_word = generate_lstm_response(history_text)\n",
        "        # Generate a more coherent response using the predicted word\n",
        "        response = f\"Based on our conversation, perhaps you're interested in '{lstm_response_word}'. Would you like to know more about Rwandan history?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Encode the input query\n",
        "    query_embedding = model.encode([query], convert_to_tensor=False).astype('float32')\n",
        "\n",
        "    # Search for the closest question\n",
        "    k = 1  # Number of nearest neighbors\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Get the distance to the closest match\n",
        "    distance = distances[0][0]\n",
        "\n",
        "    # Set a threshold for out-of-domain detection\n",
        "    DISTANCE_THRESHOLD = 1.5 # This threshold might need tuning\n",
        "\n",
        "    if distance > DISTANCE_THRESHOLD:\n",
        "        response = \"I'm sorry, I don't have information on that topic. I can help with questions about Rwandan history. What would you like to know?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Get the most similar question and its answer\n",
        "    matched_question = questions[indices[0][0]]\n",
        "    matched_answer = answers[indices[0][0]]\n",
        "\n",
        "    # Add empathetic response if emotion is detected\n",
        "    if emotion:\n",
        "        response = f\"{emotion_response} Here’s what I found: {matched_answer} Would you like to know more about Rwandan history?\"\n",
        "    else:\n",
        "        response = f\"Here’s what I found: {matched_answer} Would you like to know more about Rwandan history?\"\n",
        "\n",
        "    history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "    return response, history\n",
        "\n",
        "# Create a Gradio interface with state to maintain history\n",
        "inputs = [\n",
        "    gr.Textbox(lines=2, placeholder=\"Ask a question about Rwandan history...\"),\n",
        "    gr.State(value=[])\n",
        "]\n",
        "outputs = [\n",
        "    gr.Textbox(label=\"Response\"),\n",
        "    gr.State()\n",
        "]\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    title=\"Rwandan History Chatbot\",\n",
        "    description=\"Ask questions about Rwandan history, and the chatbot will respond based on the provided knowledge base.\"\n",
        ")\n",
        "\n",
        "# Test the chatbot with sample queries\n",
        "test_queries = [\n",
        "    \"Hello\",  # Greeting\n",
        "    \"The 1994 genocide is so heartbreaking\",  # Sadness\n",
        "    \"Tell me more\",  # Follow-up\n",
        "    \"I’m so happy to learn about Rwanda’s progress\",  # Happiness\n",
        "    \"What happened in 1973 in Rwanda?\",  # In-domain\n",
        "    \"That’s shocking!\",  # Surprise\n",
        "    \"Thanks a lot\",  # Gratitude\n",
        "    \"Goodbye\",  # Farewell\n",
        "]\n",
        "\n",
        "print(\"Testing the chatbot with sample queries:\")\n",
        "history = []\n",
        "for query in test_queries:\n",
        "    response, history = chatbot(query, history)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {response}\\n\")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "0iUfT8zXvjYr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCkJ1PJAvjV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Initialize spell checker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Define the question-answer data\n",
        "try:\n",
        "    data = pd.read_csv(\"/content/rwanda_history.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: rwanda_history.csv not found. Please upload the file.\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    data = pd.read_csv(\"/content/rwanda_history.csv\")\n",
        "\n",
        "# Extract questions and answers\n",
        "questions = data[\"Question\"].tolist()\n",
        "answers = data[\"Answer\"].tolist()\n",
        "\n",
        "# Load the sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Encode the questions\n",
        "question_embeddings = model.encode(questions, convert_to_tensor=False)\n",
        "question_embeddings = np.array(question_embeddings).astype('float32')\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = question_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(question_embeddings)\n",
        "\n",
        "# Lists for detecting greetings, gratitude, farewells, emotions, and follow-ups\n",
        "GREETINGS = [\"hello\", \"hi\", \"hey\", \"greetings\", \"good morning\", \"good afternoon\", \"good evening\"]\n",
        "GRATITUDE = [\"thank you\", \"thanks\", \"appreciate it\", \"thank you so much\", \"thanks a lot\"]\n",
        "FAREWELLS = [\"bye\", \"goodbye\", \"see you\", \"farewell\", \"take care\"]\n",
        "FOLLOW_UPS = [\"tell me more\", \"more\", \"go on\", \"continue\"]\n",
        "\n",
        "# Emotion keyword lists\n",
        "HAPPINESS = [\"happy\", \"glad\", \"excited\", \"joyful\", \"pleased\"]\n",
        "SADNESS = [\"sad\", \"heartbreaking\", \"tragic\", \"depressing\", \"devastating\"]\n",
        "ANGER = [\"angry\", \"frustrated\", \"outraged\", \"mad\", \"furious\"]\n",
        "FEAR = [\"scared\", \"terrified\", \"afraid\", \"horrified\", \"fearful\"]\n",
        "SURPRISE = [\"shocked\", \"amazed\", \"surprised\", \"astonished\", \"stunned\"]\n",
        "\n",
        "def correct_spelling(query):\n",
        "    \"\"\"Correct misspelled words in the query using pyspellchecker.\"\"\"\n",
        "    words = query.split()\n",
        "    corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in words]\n",
        "    corrected_query = \" \".join(corrected_words)\n",
        "    return corrected_query\n",
        "\n",
        "def detect_emotion(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    if any(word in query_lower for word in HAPPINESS):\n",
        "        return \"happiness\", \"I’m glad to hear that brings you happiness.\"\n",
        "    elif any(word in query_lower for word in SADNESS):\n",
        "        return \"sadness\", \"I understand, that can be truly heartbreaking.\"\n",
        "    elif any(word in query_lower for word in ANGER):\n",
        "        return \"anger\", \"I can see why that might make you feel angry.\"\n",
        "    elif any(word in query_lower for word in FEAR):\n",
        "        return \"fear\", \"I can understand why that might feel terrifying.\"\n",
        "    elif any(word in query_lower for word in SURPRISE):\n",
        "        return \"surprise\", \"That’s quite surprising, isn’t it?\"\n",
        "    return None, \"\"\n",
        "\n",
        "def is_greeting(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(greeting in query_lower for greeting in GREETINGS)\n",
        "\n",
        "def is_gratitude(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(gratitude in query_lower for gratitude in GRATITUDE)\n",
        "\n",
        "def is_farewell(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(farewell in query_lower for farewell in FAREWELLS)\n",
        "\n",
        "def is_follow_up(query):\n",
        "    query_lower = query.lower().strip()\n",
        "    return any(follow_up in query_lower for follow_up in FOLLOW_UPS)\n",
        "\n",
        "# Prepare data for LSTM training\n",
        "all_text = questions + answers + [f\"User: {q} Bot: {a}\" for q, a in zip(questions, answers)]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_text)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create sequences for training\n",
        "sequences = tokenizer.texts_to_sequences(all_text)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "# Pad sequences for X\n",
        "X = pad_sequences(sequences[:-1], maxlen=max_length, padding='post')\n",
        "\n",
        "# Prepare y by taking the next word index for each sequence in X\n",
        "y_sequences = [seq[max_length] if len(seq) > max_length else 0 for seq in sequences[1:]]\n",
        "y = tf.keras.utils.to_categorical(y_sequences, num_classes=vocab_size)\n",
        "\n",
        "# Build and train LSTM model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(vocab_size, 64, input_length=max_length),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "lstm_model.fit(X, y, epochs=50, batch_size=1, verbose=0)\n",
        "\n",
        "def generate_lstm_response(history_text):\n",
        "    sequence = tokenizer.texts_to_sequences([history_text])\n",
        "    if not sequence or not sequence[0]:\n",
        "        return \"I can provide more details. What topic would you like to explore?\"\n",
        "    padded = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "    prediction = lstm_model.predict(padded, verbose=0)\n",
        "    predicted_index = np.argmax(prediction)\n",
        "    word = tokenizer.index_word.get(predicted_index, \"details\")\n",
        "    return word\n",
        "\n",
        "def chatbot(query, history):\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    # Correct spelling in the query\n",
        "    corrected_query = correct_spelling(query)\n",
        "\n",
        "    # Detect emotion on the corrected query\n",
        "    emotion, emotion_response = detect_emotion(corrected_query)\n",
        "\n",
        "    # Check for greetings\n",
        "    if is_greeting(corrected_query):\n",
        "        response = \"Hello! I'm here to assist you with questions about Rwandan history. What would you like to learn about today?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Check for gratitude\n",
        "    if is_gratitude(corrected_query):\n",
        "        response = \"You're welcome! I'm glad I could help. Would you like to know more about Rwandan history?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Check for farewells\n",
        "    if is_farewell(corrected_query):\n",
        "        response = \"Goodbye! Feel free to come back if you have more questions about Rwandan history.\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Check for follow-up questions\n",
        "    if is_follow_up(corrected_query) and history:\n",
        "        last_turn = history[-1]\n",
        "        history_text = last_turn[0] + \" \" + last_turn[1]\n",
        "        lstm_response_word = generate_lstm_response(history_text)\n",
        "        response = f\"Based on our conversation, perhaps you're interested in '{lstm_response_word}'. Would you like to know more about Rwandan history?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    # Encode the corrected query\n",
        "    query_embedding = model.encode([corrected_query], convert_to_tensor=False).astype('float32')\n",
        "\n",
        "    # Search for the closest question\n",
        "    k = 1\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    distance = distances[0][0]\n",
        "    DISTANCE_THRESHOLD = 1.5\n",
        "\n",
        "    if distance > DISTANCE_THRESHOLD:\n",
        "        response = \"I'm sorry, I don't have information on that topic. I can help with questions about Rwandan history. What would you like to know?\"\n",
        "        history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "        return response, history\n",
        "\n",
        "    matched_question = questions[indices[0][0]]\n",
        "    matched_answer = answers[indices[0][0]]\n",
        "\n",
        "    if emotion:\n",
        "        response = f\"{emotion_response} Here’s what I found: {matched_answer} Would you like to know more about Rwandan history?\"\n",
        "    else:\n",
        "        response = f\"Here’s what I found: {matched_answer} Would you like to know more about Rwandan history?\"\n",
        "\n",
        "    history.append((\"User: \" + query, \"Bot: \" + response))\n",
        "    return response, history\n",
        "\n",
        "# Test the chatbot with sample queries, including some with misspellings\n",
        "test_queries = [\n",
        "    \"Hello\",  # Greeting\n",
        "    \"The 1994 genoside is so hertbreaking\",  # Misspelled: genoside -> genocide, hertbreaking -> heartbreaking\n",
        "    \"Tell me mor\",  # Misspelled: mor -> more\n",
        "    \"I’m so hapy to learn about Rwanda’s progres\",  # Misspelled: hapy -> happy, progres -> progress\n",
        "    \"What hapened in 1973 in Rwanda?\",  # Misspelled: hapened -> happened\n",
        "    \"That’s shoking!\",  # Misspelled: shoking -> shocking\n",
        "    \"Thanks alot\",  # Misspelled: alot -> a lot\n",
        "    \"Goodby\",  # Misspelled: Goodby -> Goodbye\n",
        "]\n",
        "\n",
        "print(\"Testing the chatbot with sample queries (including misspellings):\")\n",
        "history = []\n",
        "for query in test_queries:\n",
        "    response, history = chatbot(query, history)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {response}\\n\")\n",
        "\n",
        "# Uncomment to launch Gradio interface\n",
        "inputs = [\n",
        "    gr.Textbox(lines=2, placeholder=\"Ask a question about Rwandan history...\"),\n",
        "    gr.State(value=[])\n",
        "]\n",
        "outputs = [\n",
        "    gr.Textbox(label=\"Response\"),\n",
        "    gr.State()\n",
        "]\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    title=\"Rwandan History Chatbot\",\n",
        "    description=\"Ask questions about Rwandan history, and the chatbot will respond based on the provided knowledge base, even if you misspell words.\"\n",
        ")\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "g93uUn6yvjEJ"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}