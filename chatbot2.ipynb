{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergekamanzi/ChatBotpro/blob/main/chatbot2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d008d075",
        "outputId": "8421a47c-b36e-454c-fa57-bbce7378a845"
      },
      "source": [
        "%pip install rapidfuzz"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "009764e4"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from textblob import TextBlob\n",
        "import rapidfuzz\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/banking.csv\")  # Replace with your correct path\n",
        "df = df.dropna(subset=[\"question\", \"response\"])\n",
        "questions = df[\"question\"].tolist()\n",
        "responses = df[\"response\"].tolist()\n",
        "\n",
        "# Load model and encode questions\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "question_embeddings = model.encode(questions, convert_to_tensor=True)\n",
        "\n",
        "# Memory & context\n",
        "user_memory = {\"name\": None, \"location\": None}\n",
        "last_topic = None\n",
        "\n",
        "# Variations\n",
        "greetings = [\"hi\", \"hello\", \"hey\", \"good morning\", \"good afternoon\", \"howdy\", \"greetings\", \"what's up\", \"yo\", \"hiya\"]\n",
        "thanks = [\"thank you\", \"thanks\", \"thx\", \"appreciate\", \"much obliged\", \"cheers\", \"thankful\", \"ty\", \"gracias\", \"merci\"]\n",
        "affirmatives = [\"yes\", \"yeah\", \"sure\", \"ok\", \"alright\", \"definitely\", \"of course\", \"yea\", \"yup\", \"certainly\"]\n",
        "off_topic_keywords = [\"food\", \"love\", \"music\", \"football\", \"weather\", \"play\", \"movie\", \"hiking\", \"robot\", \"interact\"]\n",
        "\n",
        "domain_keywords = {\n",
        "    \"profit\": \"Profit and loss topics are best discussed with a financial advisor. I'm happy to assist with banking needs like savings or cards.\"\n",
        "}\n",
        "\n",
        "# Dynamic responses\n",
        "greet_replies = [\n",
        "    \"Hey {name}, how can I help you today?\", \"Nice to see you, {name}. What can I do for you?\",\n",
        "    \"Hi {name}! Ready to explore some banking options?\", \"Hello {name}, let's take care of your banking needs.\",\n",
        "    \"Welcome back, {name}! How can I assist?\", \"Hello there, {name}. What would you like help with today?\",\n",
        "    \"Hi {name}, what banking task are we tackling today?\", \"Hey {name}, how can I support you?\",\n",
        "    \"Hey {name}, let's sort your banking needs.\", \"Greetings {name}, happy to help!\"\n",
        "]\n",
        "\n",
        "thank_replies = [\n",
        "    \"You're welcome!\", \"Happy to help!\", \"Anytime!\", \"No problem, {name}!\",\n",
        "    \"Glad to assist!\", \"Always here to help!\", \"You're most welcome!\", \"With pleasure!\",\n",
        "    \"No worries!\", \"Sure thing, {name}!\"\n",
        "]\n",
        "\n",
        "off_topic_replies = [\n",
        "    \"Haha, that’s fun! I focus on banking. Want to check your balance or report a card issue?\",\n",
        "    \"Interesting! I specialize in banking. Would you like help with loans or ATM info?\",\n",
        "    \"Cool! I'm trained for banking help. Try asking about your account or a transaction.\",\n",
        "    \"That's a fun topic! I'm focused on banking though. Need help with cards or savings?\",\n",
        "    \"I love that! Let’s talk banking—need help with something like fraud or PINs?\",\n",
        "    \"Nice one! My zone is banking. Want help opening an account?\",\n",
        "    \"Haha, I feel you! I handle account inquiries and loans best.\",\n",
        "    \"That’s outside my expertise 😄. Want to know your balance instead?\",\n",
        "    \"Sounds exciting! But I’m more into balances and transfers!\",\n",
        "    \"I wish I could help with that! But I’m your banking assistant 😄\"\n",
        "]\n",
        "\n",
        "fallback_replies = [\n",
        "    \"I'm here for your banking needs—like loans, fraud, or PIN help. What do you need?\",\n",
        "    \"Sorry, I didn’t quite get that. Would you like help with opening an account?\",\n",
        "    \"Interesting! I focus on banking. Try asking about ATM, transfers, or cards.\",\n",
        "    \"Hmm, I’m not sure. But I’m great at deposits, balances, and reports!\",\n",
        "    \"I’m better at banking questions. Want to check your balance or card status?\",\n",
        "    \"Could you rephrase that? I can help with things like savings or fraud issues.\",\n",
        "    \"Let’s get back to banking. Do you want to know about transfers or cards?\",\n",
        "    \"Sorry, didn’t follow. But I can help with loans, accounts, or deposits.\",\n",
        "    \"That’s a bit unclear. Do you need help with your account or a transaction?\",\n",
        "    \"Let’s stay on topic—banking help like ATM, PIN, or transfers coming up?\"\n",
        "]\n",
        "\n",
        "# Helpers\n",
        "def correct_spelling(text):\n",
        "    return str(TextBlob(text).correct())\n",
        "\n",
        "def update_memory(text):\n",
        "    global user_memory\n",
        "    name_match = re.search(r\"(my name is|i am|i'm|this is|call me|you can call me|it's|they call me|name's)\\s+(\\w+)\", text.lower())\n",
        "    name_correction = re.search(r\"not (\\w+)\", text.lower())\n",
        "    loc_match = re.search(r\"(i live in|i'm from|i stay in)\\s+([a-zA-Z\\s]+)\", text.lower())\n",
        "\n",
        "    response = \"\"\n",
        "    if name_match:\n",
        "        user_memory[\"name\"] = name_match.group(2).capitalize()\n",
        "        response += f\"Nice to meet you, {user_memory['name']}! \"\n",
        "    elif name_correction and user_memory[\"name\"]:\n",
        "        corrected_name = name_correction.group(1).capitalize()\n",
        "        user_memory[\"name\"] = corrected_name\n",
        "        response += f\"Got it! I’ve updated your name to {corrected_name}. \"\n",
        "\n",
        "    if loc_match:\n",
        "        user_memory[\"location\"] = loc_match.group(2).strip().capitalize()\n",
        "        response += f\"{user_memory['location']} sounds like a great place. \"\n",
        "\n",
        "    if response:\n",
        "        response += \"How can I assist you with banking today?\"\n",
        "        return response\n",
        "    return None\n",
        "\n",
        "def personalize(text):\n",
        "    return text.replace(\"{name}\", user_memory[\"name\"] if user_memory[\"name\"] else \"there\")\n",
        "\n",
        "def detect_intent(text):\n",
        "    text = text.lower()\n",
        "    for g in greetings:\n",
        "        if rapidfuzz.fuzz.partial_ratio(g, text) > 90:\n",
        "            return \"greet\"\n",
        "    for t in thanks:\n",
        "        if rapidfuzz.fuzz.partial_ratio(t, text) > 90:\n",
        "            return \"thanks\"\n",
        "    for w in off_topic_keywords:\n",
        "        if w in text:\n",
        "            return \"off_topic\"\n",
        "    if \"remember\" in text and \"name\" in text:\n",
        "        return \"check_name\"\n",
        "    if any(word in text for word in affirmatives):\n",
        "        return \"confirm\"\n",
        "    if \"talk like a human\" in text or \"interact like a human\" in text:\n",
        "        return \"human_mode\"\n",
        "    if \"more about\" in text or \"tell me more\" in text:\n",
        "        return \"clarify\"\n",
        "    return \"question\"\n",
        "\n",
        "# Main chatbot loop\n",
        "def chatbot():\n",
        "    global last_topic\n",
        "    print(\"🤖 Human-Like Banking Chatbot is ready! Type 'exit' to quit.\\n\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Chatbot: Goodbye! It was a pleasure assisting you.\")\n",
        "            break\n",
        "\n",
        "        corrected = correct_spelling(user_input)\n",
        "        memory_reply = update_memory(corrected)\n",
        "        if memory_reply:\n",
        "            print(\"Chatbot:\", memory_reply)\n",
        "            continue\n",
        "\n",
        "        for keyword in domain_keywords:\n",
        "            if keyword in corrected.lower():\n",
        "                print(\"Chatbot:\", domain_keywords[keyword])\n",
        "                continue\n",
        "\n",
        "        intent = detect_intent(corrected)\n",
        "\n",
        "        if intent == \"greet\":\n",
        "            print(\"Chatbot:\", personalize(random.choice(greet_replies)))\n",
        "            continue\n",
        "        if intent == \"thanks\":\n",
        "            print(\"Chatbot:\", personalize(random.choice(thank_replies)))\n",
        "            continue\n",
        "        if intent == \"off_topic\":\n",
        "            print(\"Chatbot:\", random.choice(off_topic_replies))\n",
        "            continue\n",
        "        if intent == \"check_name\":\n",
        "            if user_memory[\"name\"]:\n",
        "                print(f\"Chatbot: Yes! You told me your name is {user_memory['name']}.\")\n",
        "            else:\n",
        "                print(\"Chatbot: I don’t think you’ve told me your name yet. Try saying 'My name is ...'.\")\n",
        "            continue\n",
        "        if intent == \"human_mode\":\n",
        "            print(\"Chatbot: Absolutely! I’m here to chat in a friendly way and support your banking needs 😊\")\n",
        "            continue\n",
        "        if intent == \"confirm\":\n",
        "            if last_topic:\n",
        "                print(f\"Chatbot: Continuing from our last topic: **{last_topic}**. What exactly would you like to know?\")\n",
        "            else:\n",
        "                print(\"Chatbot: Sure! Can you share more about what you'd like help with?\")\n",
        "            continue\n",
        "        if intent == \"clarify\":\n",
        "            if last_topic:\n",
        "                print(f\"Chatbot: Here's more on your last topic: **{last_topic}**. Do you want to open an account or understand account types?\")\n",
        "            else:\n",
        "                print(\"Chatbot: Can you clarify which topic you'd like to dive into more?\")\n",
        "            continue\n",
        "\n",
        "        # Semantic similarity\n",
        "        user_emb = model.encode(corrected, convert_to_tensor=True)\n",
        "        scores = util.cos_sim(user_emb, question_embeddings)[0]\n",
        "        best_idx = scores.argmax().item()\n",
        "        best_score = scores[best_idx].item()\n",
        "\n",
        "        if best_score > 0.6:\n",
        "            last_topic = questions[best_idx]\n",
        "            print(\"Chatbot:\", personalize(responses[best_idx]))\n",
        "        else:\n",
        "            print(\"Chatbot:\", random.choice(fallback_replies))\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install evaluate\n",
        "%pip install optuna\n",
        "%pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QgkWrTEtDSI",
        "outputId": "7c28e338-c275-4b86-dd58-11fa12cd6437"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TESTING\n"
      ],
      "metadata": {
        "id": "-COok1XOyryw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# os.environ[\"WANDB_DISABLED\"] = \"true\" # Keep this as a fallback, but try explicit init\n",
        "import wandb # Import wandb\n",
        "wandb.init(mode='disabled') # Explicitly disable wandb\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer, util, InputExample, losses\n",
        "from textblob import TextBlob\n",
        "import rapidfuzz\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set NLTK data path to a writable directory\n",
        "nltk.data.path.append('/root/nltk_data')\n",
        "\n",
        "# Download NLTK resources\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', download_dir='/root/nltk_data')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab', download_dir='/root/nltk_data')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', download_dir='/root/nltk_data')\n",
        "\n",
        "# Import dataset using pandas\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/banking.csv\")\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File not found. Please check the file path.\")\n",
        "    exit()\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(\"Error: The CSV file is empty.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Tokenization, normalization, and handling missing values.\"\"\"\n",
        "    df = df.dropna(subset=[\"question\", \"response\"])\n",
        "    df = df.fillna(\"\")\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def clean_text(text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    df['question_cleaned'] = df['question'].apply(clean_text)\n",
        "    df['response_cleaned'] = df['response'].apply(clean_text)\n",
        "    return df\n",
        "\n",
        "# Dataset Visualization\n",
        "def visualize_data(df, output_dir=\"plots\"):\n",
        "    \"\"\"Generate visualizations for the dataset.\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    df['question_length'] = df['question'].apply(lambda x: len(x.split()))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df['question_length'], bins=30)\n",
        "    plt.title(\"Distribution of Question Lengths\")\n",
        "    plt.xlabel(\"Number of Words\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show() # Changed to show() for inline display\n",
        "\n",
        "    df['response_length'] = df['response'].apply(lambda x: len(x.split()))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df['response_length'], bins=30)\n",
        "    plt.title(\"Distribution of Response Lengths\")\n",
        "    plt.xlabel(\"Number of Words\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show() # Changed to show() for inline display\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=df['question_length'], y=df['response_length'])\n",
        "    plt.title(\"Question vs Response Length Correlation\")\n",
        "    plt.xlabel(\"Question Length\")\n",
        "    plt.ylabel(\"Response Length\")\n",
        "    plt.show() # Changed to show() for inline display\n",
        "\n",
        "# NLP Metrics Evaluation\n",
        "def evaluate_metrics(true_responses, pred_responses):\n",
        "    \"\"\"Calculate BLEU, F1, Accuracy, and Precision for SentenceTransformer.\"\"\"\n",
        "    bleu_scores = []\n",
        "    f1_scores = []\n",
        "    precision_scores = []\n",
        "    accuracy_scores = []\n",
        "\n",
        "    for true, pred in zip(true_responses, pred_responses):\n",
        "        true_tokens = true.split()\n",
        "        pred_tokens = pred.split()\n",
        "\n",
        "        bleu = sentence_bleu([true_tokens], pred_tokens)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # Using classification metrics on token sets (simplified)\n",
        "        true_set = set(true_tokens)\n",
        "        pred_set = set(pred_tokens)\n",
        "        y_true = [1 if word in true_set else 0 for word in true_set.union(pred_set)]\n",
        "        y_pred = [1 if word in pred_set else 0 for word in true_set.union(pred_set)]\n",
        "\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "        f1_scores.append(f1)\n",
        "        precision_scores.append(precision)\n",
        "        accuracy_scores.append(accuracy)\n",
        "\n",
        "    # Perplexity calculation requires a language model like GPT-2,\n",
        "    # which is complex and not directly relevant to the SentenceTransformer's core task (embedding).\n",
        "    # Omitting perplexity for now to focus on the SentenceTransformer training.\n",
        "    # If needed, the GPT-2 part can be added back and handled separately.\n",
        "\n",
        "    return {\n",
        "        \"BLEU\": np.mean(bleu_scores),\n",
        "        \"F1\": np.mean(f1_scores),\n",
        "        \"Precision\": np.mean(precision_scores),\n",
        "        \"Accuracy\": np.mean(accuracy_scores)\n",
        "    }\n",
        "\n",
        "# Plot Training and Test Graphs\n",
        "def plot_training_curves(train_losses, test_losses, output_dir=\"plots\"):\n",
        "    \"\"\"Plot training and test loss curves.\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss')\n",
        "    plt.title(\"Training and Test Loss Over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Contrastive Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{output_dir}/training_test_loss.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Hyperparameter Tuning for SentenceTransformer\n",
        "def train_model(df, hyperparameters, output_dir=\"plots\"):\n",
        "    \"\"\"Train SentenceTransformer with hyperparameter tuning and plot losses.\"\"\"\n",
        "    best_score = -float('inf')\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    # Train/test split\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    train_questions = train_df['question_cleaned'].tolist()\n",
        "    train_responses = train_df['response_cleaned'].tolist()\n",
        "    test_questions = test_df['question_cleaned'].tolist()\n",
        "    test_responses = test_df['response_cleaned'].tolist()\n",
        "\n",
        "    for lr in hyperparameters['learning_rates']:\n",
        "        for batch_size in hyperparameters['batch_sizes']:\n",
        "            for optimizer_name in hyperparameters['optimizers']:\n",
        "                for epoch in hyperparameters['epochs']:\n",
        "                    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "                    train_examples = [InputExample(texts=[q, r], label=1.0) for q, r in zip(train_questions, train_responses)]\n",
        "                    test_examples = [InputExample(texts=[q, r], label=1.0) for q, r in zip(test_questions, test_responses)]\n",
        "\n",
        "                    train_dataloader = DataLoader(train_examples, batch_size=batch_size, shuffle=True)\n",
        "                    test_dataloader = DataLoader(test_examples, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "                    # Define loss function\n",
        "                    train_loss = losses.ContrastiveLoss(model=model)\n",
        "\n",
        "                    # Train the model using model.fit()\n",
        "                    model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "                              epochs=epoch,\n",
        "                              optimizer_params={'lr': lr},\n",
        "                              # scheduler='warmuprelativisticint', # Example scheduler\n",
        "                              # warmup_steps=100, # Example warmup steps\n",
        "                              evaluator=None, # Can add an evaluator for evaluation during training\n",
        "                              output_path=None, # Can specify a path to save the trained model\n",
        "                              show_progress_bar=True # Show progress bar\n",
        "                             )\n",
        "\n",
        "                    # Evaluate on test set after training\n",
        "                    test_embeddings = model.encode(test_questions, convert_to_tensor=True)\n",
        "                    pred_responses = [test_responses[np.argmax(util.cos_sim(model.encode(q, convert_to_tensor=True), test_embeddings).cpu().numpy()[0])] for q in test_questions]\n",
        "                    metrics = evaluate_metrics(test_responses, pred_responses)\n",
        "                    score = metrics['BLEU'] # Using BLEU as the scoring metric for hyperparameter tuning\n",
        "\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_params = {'lr': lr, 'batch_size': batch_size, 'optimizer': optimizer_name, 'epoch': epoch}\n",
        "                        best_model = model\n",
        "                        print(f\"New best BLEU: {score:.4f}, Params: {best_params}\")\n",
        "                        print(f\"Metrics: {metrics}\")\n",
        "\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "    return best_model, train_questions, train_responses, test_questions, test_responses\n",
        "\n",
        "# Preprocess and visualize data\n",
        "df = preprocess_data(df)\n",
        "visualize_data(df)\n",
        "\n",
        "# Hyperparameter configurations\n",
        "hyperparameters = {\n",
        "    'learning_rates': [1e-5], # Reduced for faster execution\n",
        "    'batch_sizes': [16], # Reduced for faster execution\n",
        "    'optimizers': ['Adam'], # Reduced for faster execution\n",
        "    'epochs': [1] # Reduced for faster execution\n",
        "}\n",
        "\n",
        "# Train model\n",
        "model, train_questions, train_responses, test_questions, test_responses = train_model(df, hyperparameters)\n",
        "questions = df['question_cleaned'].tolist()\n",
        "responses = df['response_cleaned'].tolist()\n",
        "question_embeddings = model.encode(questions, convert_to_tensor=True)\n",
        "\n",
        "# Calculate and print metrics for the best model on the test set\n",
        "print(\"\\nEvaluation Metrics for the best model on the test set:\")\n",
        "test_embeddings = model.encode(test_questions, convert_to_tensor=True)\n",
        "pred_responses = [test_responses[np.argmax(util.cos_sim(model.encode(q, convert_to_tensor=True), test_embeddings).cpu().numpy()[0])] for q in test_questions]\n",
        "metrics = evaluate_metrics(test_responses, pred_responses)\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Memory & context\n",
        "user_memory = {\"name\": None, \"location\": None}\n",
        "last_topic = None\n",
        "\n",
        "# Variations\n",
        "greetings = [\"hi\", \"hello\", \"hey\", \"good morning\", \"good afternoon\", \"howdy\", \"greetings\", \"what's up\", \"yo\", \"hiya\"]\n",
        "thanks = [\"thank you\", \"thanks\", \"thx\", \"appreciate\", \"much obliged\", \"cheers\", \"thankful\", \"ty\", \"gracias\", \"merci\"]\n",
        "affirmatives = [\"yes\", \"yeah\", \"sure\", \"ok\", \"alright\", \"definitely\", \"of course\", \"yea\", \"yup\", \"certainly\"]\n",
        "off_topic_keywords = [\"food\", \"love\", \"music\", \"football\", \"weather\", \"play\", \"movie\", \"hiking\", \"robot\", \"interact\"]\n",
        "\n",
        "domain_keywords = {\n",
        "    \"profit\": \"Profit and loss topics are best discussed with a financial advisor. I'm happy to assist with banking needs like savings or cards.\"\n",
        "}\n",
        "\n",
        "# Dynamic responses\n",
        "greet_replies = [\n",
        "    \"Hey {name}, how can I help you today?\", \"Nice to see you, {name}. What can I do for you?\",\n",
        "    \"Hi {name}! Ready to explore some banking options?\", \"Hello {name}, let's take care of your banking needs.\",\n",
        "    \"Welcome back, {name}! How can I assist?\", \"Hello there, {name}. What would you like help with today?\",\n",
        "    \"Hi {name}, what banking task are we tackling today?\", \"Hey {name}, how can I support you?\",\n",
        "    \"Hey {name}, let's sort your banking needs.\", \"Greetings {name}, happy to help!\"\n",
        "]\n",
        "\n",
        "thank_replies = [\n",
        "    \"You're welcome!\", \"Happy to help!\", \"Anytime!\", \"No problem, {name}!\",\n",
        "    \"Glad to assist!\", \"Always here to help!\", \"You're most welcome!\", \"With pleasure!\",\n",
        "    \"No worries!\", \"Sure thing, {name}!\"\n",
        "]\n",
        "\n",
        "off_topic_replies = [\n",
        "    \"Haha, that’s fun! I focus on banking. Want to check your balance or report a card issue?\",\n",
        "    \"Interesting! I specialize in banking. Would you like help with loans or ATM info?\",\n",
        "    \"Cool! I'm trained for banking help. Try asking about your account or a transaction.\",\n",
        "    \"That's a fun topic! I'm focused on banking though. Need help with cards or savings?\",\n",
        "    \"I love that! Let’s talk banking—need help with something like fraud or PINs?\",\n",
        "    \"Nice one! My zone is banking. Want help opening an account?\",\n",
        "    \"Haha, I feel you! I handle account inquiries and loans best.\",\n",
        "    \"That’s outside my expertise 😄. Want to know your balance instead?\",\n",
        "    \"Sounds exciting! But I’m more into balances and transfers!\",\n",
        "    \"I wish I could help with that! But I’m your banking assistant 😄\"\n",
        "]\n",
        "\n",
        "fallback_replies = [\n",
        "    \"I'm here for your banking needs—like loans, fraud, or PIN help. What do you need?\",\n",
        "    \"Sorry, I didn’t quite get that. Would you like help with opening an account?\",\n",
        "    \"Interesting! I focus on banking. Try asking about ATM, transfers, or cards.\",\n",
        "    \"Hmm, I’m not sure. But I’m great at deposits, balances, and reports!\",\n",
        "    \"I’m better at banking questions. Want to check your balance or card status?\",\n",
        "    \"Could you rephrase that? I can help with things like savings or fraud issues.\",\n",
        "    \"Let’s get back to banking. Do you want to know about transfers or cards?\",\n",
        "    \"Sorry, didn’t follow. But I can help with loans, accounts, or deposits.\",\n",
        "    \"That’s a bit unclear. Do you need help with your account or a transaction?\",\n",
        "    \"Let’s stay on topic—banking help like ATM, PIN, or transfers coming up?\"\n",
        "]\n",
        "\n",
        "# Helpers\n",
        "def correct_spelling(text):\n",
        "    return str(TextBlob(text).correct())\n",
        "\n",
        "def update_memory(text):\n",
        "    global user_memory\n",
        "    name_match = re.search(r\"(my name is|i am|i'm|this is|call me|you can call me|it's|they call me|name's)\\s+(\\w+)\", text.lower())\n",
        "    name_correction = re.search(r\"not (\\w+)\", text.lower())\n",
        "    loc_match = re.search(r\"(i live in|i'm from|i stay in)\\s+([a-zA-Z\\s]+)\", text.lower())\n",
        "\n",
        "    response = \"\"\n",
        "    if name_match:\n",
        "        user_memory[\"name\"] = name_match.group(2).capitalize()\n",
        "        response += f\"Nice to meet you, {user_memory['name']}! \"\n",
        "    elif name_correction and user_memory[\"name\"]:\n",
        "        corrected_name = name_correction.group(1).capitalize()\n",
        "        user_memory[\"name\"] = corrected_name\n",
        "        response += f\"Got it! I’ve updated your name to {corrected_name}. \"\n",
        "\n",
        "    if loc_match:\n",
        "        user_memory[\"location\"] = loc_match.group(2).strip().capitalize()\n",
        "        response += f\"{user_memory['location']} sounds like a great place. \"\n",
        "\n",
        "    if response:\n",
        "        response += \"How can I assist you with banking today?\"\n",
        "        return response\n",
        "    return None\n",
        "\n",
        "def personalize(text):\n",
        "    return text.replace(\"{name}\", user_memory[\"name\"] if user_memory[\"name\"] else \"there\")\n",
        "\n",
        "def detect_intent(text):\n",
        "    text = text.lower()\n",
        "    for g in greetings:\n",
        "        if rapidfuzz.fuzz.partial_ratio(g, text) > 90:\n",
        "            return \"greet\"\n",
        "    for t in thanks:\n",
        "        if rapidfuzz.fuzz.partial_ratio(t, text) > 90:\n",
        "            return \"thanks\"\n",
        "    for w in off_topic_keywords:\n",
        "        if w in text:\n",
        "            return \"off_topic\"\n",
        "    if \"remember\" in text and \"name\" in text:\n",
        "        return \"check_name\"\n",
        "    if any(word in text for word in affirmatives):\n",
        "        return \"confirm\"\n",
        "    if \"talk like a human\" in text or \"interact like a human\" in text:\n",
        "            return \"human_mode\"\n",
        "    if \"more about\" in text or \"tell me more\" in text:\n",
        "        return \"clarify\"\n",
        "    return \"question\"\n",
        "\n",
        "# Main chatbot loop\n",
        "def chatbot():\n",
        "    global last_topic\n",
        "    print(\"🤖 Human-Like Banking Chatbot is ready! Type 'exit' to quit.\\n\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Chatbot: Goodbye! It was a pleasure assisting you.\")\n",
        "            break\n",
        "\n",
        "        corrected = correct_spelling(user_input)\n",
        "        memory_reply = update_memory(corrected)\n",
        "        if memory_reply:\n",
        "            print(\"Chatbot:\", memory_reply)\n",
        "            continue\n",
        "\n",
        "        for keyword in domain_keywords:\n",
        "            if keyword in corrected.lower():\n",
        "                print(\"Chatbot:\", domain_keywords[keyword])\n",
        "                continue\n",
        "\n",
        "        intent = detect_intent(corrected)\n",
        "\n",
        "        if intent == \"greet\":\n",
        "            print(\"Chatbot:\", personalize(random.choice(greet_replies)))\n",
        "            continue\n",
        "        if intent == \"thanks\":\n",
        "            print(\"Chatbot:\", personalize(random.choice(thank_replies)))\n",
        "            continue\n",
        "        if intent == \"off_topic\":\n",
        "            print(\"Chatbot:\", random.choice(off_topic_replies))\n",
        "            continue\n",
        "        if intent == \"check_name\":\n",
        "            if user_memory[\"name\"]:\n",
        "                print(f\"Chatbot: Yes! You told me your name is {user_memory['name']}.\")\n",
        "            else:\n",
        "                print(\"Chatbot: I don’t think you’ve told me your name yet. Try saying 'My name is ...'.\")\n",
        "            continue\n",
        "        if intent == \"human_mode\":\n",
        "            print(\"Chatbot: Absolutely! I’m here to chat in a friendly way and support your banking needs 😊\")\n",
        "            continue\n",
        "        if intent == \"confirm\":\n",
        "            if last_topic:\n",
        "                print(f\"Chatbot: Continuing from our last topic: **{last_topic}**. What exactly would you like to know?\")\n",
        "            else:\n",
        "                print(\"Chatbot: Sure! Can you share more about what you'd like help with?\")\n",
        "            continue\n",
        "        if intent == \"clarify\":\n",
        "            if last_topic:\n",
        "                print(f\"Chatbot: Here's more on your last topic: **{last_topic}**. Do you want to open an account or understand account types?\")\n",
        "            else:\n",
        "                print(\"Chatbot: Can you clarify which topic you'd like to dive into more?\")\n",
        "            continue\n",
        "\n",
        "        user_emb = model.encode(corrected, convert_to_tensor=True)\n",
        "        scores = util.cos_sim(user_emb, question_embeddings)[0]\n",
        "        best_idx = scores.argmax().item()\n",
        "        best_score = scores[best_idx].item()\n",
        "\n",
        "        if best_score > 0.6:\n",
        "            last_topic = questions[best_idx]\n",
        "            print(\"Chatbot:\", personalize(responses[best_idx]))\n",
        "        else:\n",
        "            print(\"Chatbot:\", random.choice(fallback_replies))\n",
        "\n",
        "# Run the chatbot\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot()"
      ],
      "metadata": {
        "id": "adEPD1GINYH4"
      },
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}